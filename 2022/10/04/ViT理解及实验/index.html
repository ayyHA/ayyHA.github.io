

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="基础理解及简单实验">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <meta name="description" content="基础理解及简单实验">
<meta property="og:type" content="article">
<meta property="og:title" content="ViT理解及实验">
<meta property="og:url" content="http://example.com/2022/10/04/ViT%E7%90%86%E8%A7%A3%E5%8F%8A%E5%AE%9E%E9%AA%8C/index.html">
<meta property="og:site_name" content="ayyHA&#39;s blog">
<meta property="og:description" content="基础理解及简单实验">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://qny.ayyha.store/ViT-Paper-Result.png">
<meta property="og:image" content="http://qny.ayyha.store/Details%20of%20ViT%20variants.png">
<meta property="og:image" content="http://qny.ayyha.store/ViT-overview.png">
<meta property="og:image" content="http://qny.ayyha.store/ViT%20encoder%20block.png">
<meta property="og:image" content="http://qny.ayyha.store/MLP-HEAD.png">
<meta property="og:image" content="http://qny.ayyha.store/ViT%20Execution%20Process.png">
<meta property="og:image" content="http://qny.ayyha.store/representation%20structure%20of%20ViTs%20and%20CNN.png">
<meta property="og:image" content="http://qny.ayyha.store/representation%20structure%20between%20ViTs%20and%20CNN.png">
<meta property="og:image" content="http://qny.ayyha.store/Local%20and%20Global%20information%20that%20ViT%20learned.png">
<meta property="og:image" content="http://qny.ayyha.store/Local%20and%20Global%20information%20which%20no%20pretrained%20ViT%20learned.png">
<meta property="og:image" content="http://qny.ayyha.store/Local%20and%20Global%20ViTs%20Representations%20compared%20to%20ResNets.png">
<meta property="og:image" content="http://qny.ayyha.store/ViT%20and%20ResNet%20ERF.png">
<meta property="og:image" content="http://qny.ayyha.store/Pre-residual%20receptive%20fields%20of%20all%20ViT-B32%20sublayers.png">
<meta property="og:image" content="http://qny.ayyha.store/ratio%20of%20norms.png">
<meta property="og:image" content="http://qny.ayyha.store/ViT%20remove%20a%20block%27s%20skip%20connection.png">
<meta property="og:image" content="http://qny.ayyha.store/spatial%20location%20of%20ViT%20%26%20ResNet.png">
<meta property="og:image" content="http://qny.ayyha.store/ViT%20use%20GAP%20to%20classification.png">
<meta property="article:published_time" content="2022-10-04T03:03:17.000Z">
<meta property="article:modified_time" content="2022-10-13T09:48:41.081Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="cv">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://qny.ayyha.store/ViT-Paper-Result.png">
  
  <title>ViT理解及实验 - ayyHA&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"9c7ed39aa5906acb06d9f9cb7df236ae","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ayyHA</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="ViT理解及实验">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-10-04 11:03" pubdate>
        2022年10月4日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      8.7k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      27 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">ViT理解及实验</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：3 小时前
                
              </p>
            
            <div class="markdown-body">
              <h2 id="引言"><a class="markdownIt-Anchor" href="#引言"></a> 引言</h2>
<p>ViT(Vision Transformer),是ICLR 2021上的一篇<strong>AN IMAGE IS WORTH 16x16 WORDS:TRANSFORMER FOR IMAGE RECOGNITION AT SCALE</strong>论文里提及的模型,将<strong>transformer应用于image classification</strong>上,并在Google的JFT-300M数据集进行预训练后,在ImageNet-1k上做分类达到当时的SOTA!<br />
<img src="http://qny.ayyha.store/ViT-Paper-Result.png" srcset="/img/loading.gif" lazyload alt="ViT-Paper-Result" /></p>
<p>因此,本文将通过结构结合代码进行介绍,并于文末将采用与预训练好的模型应用于花分类数据集上进行实验.</p>
<div class="note note-info">
            <p>注:本文采用的是ViT-B/16模型进行解析,B-&gt;Base 16-&gt;patch size:16*16,输入图片的shape是(224,224,3)</p>
          </div>
<p><img src="http://qny.ayyha.store/Details%20of%20ViT%20variants.png" srcset="/img/loading.gif" lazyload alt="Details of ViT variants" /></p>
<p>原论文链接:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">AN IMAGE IS WORTH 16x16 WORDS:TRANSFORMER IMAGE RECOGNITION AT SCALE</a><br />
<a target="_blank" rel="noopener" href="https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz">花分类数据集下载</a></p>
<h2 id="vit整体结构"><a class="markdownIt-Anchor" href="#vit整体结构"></a> ViT整体结构</h2>
<p>首先看下ViT的整体结构图:<br />
<img src="http://qny.ayyha.store/ViT-overview.png" srcset="/img/loading.gif" lazyload alt="ViT-overview" /><br />
整个ViT由以下三部分组成:</p>
<ul>
<li>Linear Projection of Flattened Patches -&gt; 转换为transformer接受的输入</li>
<li>Transformer Encoder -&gt; 进入Encoder学习图片的相关性,对应特征</li>
<li>MLP Head -&gt; 做分类,其实就是全连接层</li>
</ul>
<h3 id="linear-projection-of-flattened-patches"><a class="markdownIt-Anchor" href="#linear-projection-of-flattened-patches"></a> Linear Projection of Flattened Patches</h3>
<h4 id="patch-embedding"><a class="markdownIt-Anchor" href="#patch-embedding"></a> patch embedding</h4>
<p>根据先前介绍的transformer的知识,我们输入的应该是token sequence,如由词向量组成的矩阵,row的个数表示词的个数,column宽度表示词的dimension.<br />
因此输入图片是不符合要求的,我们需要将图片处理成<strong>对应的序列</strong>才行.将图片切割成一小块一小块的patch,再将patches延展成一维的(获得patch_num),再将每个patch通过线性变换映射到对应的维度(patch_dim),即完成了patch embedding的过程.最终得到的参数为:[patch_num,patch_dim]<br />
其实上述过程,用卷积的思想解释就是<strong>kernel=16x16,stride=16,采用768个卷积核进行卷积计算,即可以将[224,224,3]-&gt;[14,14,768]</strong>,然后再通过torch的flatten处理就可以得到[196,768]</p>
<div class="note note-info">
            <p>注:这里的<strong>768</strong>并非通过如16*16*3计算出来的,而是作者规定的dimension,如ViT-H/16,其patch_dim是1280.</p>
          </div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PatchEmbed</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    2D Image to Patch Embedding</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, img_size=<span class="hljs-number">224</span>, patch_size=<span class="hljs-number">16</span>, in_c=<span class="hljs-number">3</span>, embed_dim=<span class="hljs-number">768</span>, norm_layer=<span class="hljs-literal">None</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        img_size = (img_size, img_size)<br>        patch_size = (patch_size, patch_size)<br>        self.img_size = img_size<br>        self.patch_size = patch_size<br>        self.grid_size = (img_size[<span class="hljs-number">0</span>] // patch_size[<span class="hljs-number">0</span>], img_size[<span class="hljs-number">1</span>] // patch_size[<span class="hljs-number">1</span>])<br>        self.num_patches = self.grid_size[<span class="hljs-number">0</span>] * self.grid_size[<span class="hljs-number">1</span>]<br><br>        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size) <span class="hljs-comment"># 这里所复现的代码用的是卷积的形式,原文的代码不是卷积</span><br>        self.norm = norm_layer(embed_dim) <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">else</span> nn.Identity() <span class="hljs-comment"># 这里的Identity指的是不对输入做任何修改直接输出的意思</span><br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        B, C, H, W = x.shape <span class="hljs-comment"># x.shape = [batch_size,channel,height,width]</span><br>        <span class="hljs-keyword">assert</span> H == self.img_size[<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> W == self.img_size[<span class="hljs-number">1</span>], \<br>            <span class="hljs-string">f&quot;Input image size (<span class="hljs-subst">&#123;H&#125;</span>*<span class="hljs-subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="hljs-subst">&#123;self.img_size[<span class="hljs-number">0</span>]&#125;</span>*<span class="hljs-subst">&#123;self.img_size[<span class="hljs-number">1</span>]&#125;</span>).&quot;</span><br><br>        <span class="hljs-comment"># flatten: [B, C, H, W] -&gt; [B, C, HW] </span><br>        <span class="hljs-comment"># transpose: [B, C, HW] -&gt; [B, HW, C] 即[B,196,768]</span><br>        x = self.proj(x).flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        x = self.norm(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<div class="note note-info">
            <p>注: 代码中的类继承了nn.Module,这里面重写了魔法方法__call__,该方法里面调用了forward方法,因此子类重载forward可使得该方法通过<strong>类实例化对象如普通方法般调用</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">patchEmbed = PatchEmbed(para1,para2,...)<br>x = torch.rand(B,C,H,W)<br>patchEmbed(x) <span class="hljs-comment"># 于此会调用__call__方法,因而调用自己重载后的forward方法</span><br></code></pre></td></tr></table></figure>
          </div>
<h4 id="cls-token"><a class="markdownIt-Anchor" href="#cls-token"></a> CLS token</h4>
<p>ViT中的分类采用的是Bert中的CLS的思想来进行的,因此我们的patchEmbedding需要变换成**[196+1,768]**,即多出一行用于分类,这个参数由网络学习得到.其与patchEmbedding是concat的关系</p>
<h4 id="positional-embedding"><a class="markdownIt-Anchor" href="#positional-embedding"></a> positional embedding</h4>
<p>在transformer里讲过,我们输入的序列缺乏位置信息,因此需要增加positional embedding来使得其位置信息得以保持.在代码中,我们的位置信息是通过模型训练获得的(nn.Parameter()),其与patch embedding是直接add的</p>
<h3 id="encoder-block"><a class="markdownIt-Anchor" href="#encoder-block"></a> encoder block</h3>
<p>流程如下图所示:<br />
<img src="http://qny.ayyha.store/ViT%20encoder%20block.png" srcset="/img/loading.gif" lazyload alt="ViT Encoder Block" /><br />
关于Multi-Head Attention层相关的部分跟transformer的一样,这里不赘述.以下是Multi-Head Attention的代码解析部分:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Attention</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,</span></span><br><span class="hljs-params"><span class="hljs-function">                 dim,   <span class="hljs-comment"># 输入token的dim(768)</span></span></span><br><span class="hljs-params"><span class="hljs-function">                 num_heads=<span class="hljs-number">8</span>, <span class="hljs-comment"># 有几个头(multi的head)</span></span></span><br><span class="hljs-params"><span class="hljs-function">                 qkv_bias=<span class="hljs-literal">False</span>, <span class="hljs-comment"># 偏移量</span></span></span><br><span class="hljs-params"><span class="hljs-function">                 qk_scale=<span class="hljs-literal">None</span>, <span class="hljs-comment"># 放缩量</span></span></span><br><span class="hljs-params"><span class="hljs-function">                 attn_drop_ratio=<span class="hljs-number">0.</span>, <span class="hljs-comment"># attention公式执行完后的dropout比例</span></span></span><br><span class="hljs-params"><span class="hljs-function">                 proj_drop_ratio=<span class="hljs-number">0.</span></span>):</span> <span class="hljs-comment"># 一个encoder block执行完后的drop比例 </span><br>        <span class="hljs-built_in">super</span>(Attention, self).__init__()<br>        self.num_heads = num_heads<br>        head_dim = dim // num_heads <span class="hljs-comment"># 每个头对应的维度</span><br>        self.scale = qk_scale <span class="hljs-keyword">or</span> head_dim ** -<span class="hljs-number">0.5</span><br>        self.qkv = nn.Linear(dim, dim * <span class="hljs-number">3</span>, bias=qkv_bias) <span class="hljs-comment"># 将QKV三个一次性一起生成</span><br>        self.attn_drop = nn.Dropout(attn_drop_ratio)<br>        self.proj = nn.Linear(dim, dim)<br>        self.proj_drop = nn.Dropout(proj_drop_ratio)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># [batch_size, num_patches + 1, total_embed_dim]</span><br>        B, N, C = x.shape<br><br>        <span class="hljs-comment"># qkv(): -&gt; [batch_size, num_patches + 1, 3 * total_embed_dim] QKV三个矩阵一起的(相当于沿列方向concat,[B,196,768*3])</span><br>        <span class="hljs-comment"># reshape: -&gt; [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head] </span><br>        <span class="hljs-comment"># permute: -&gt; [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head] Q,K,V三个矩阵分出来,并且将其各分成num_heads个小矩阵</span><br>        qkv = self.qkv(x).reshape(B, N, <span class="hljs-number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>)<br>        <span class="hljs-comment"># [batch_size, num_heads, num_patches + 1, embed_dim_per_head]</span><br>        q, k, v = qkv[<span class="hljs-number">0</span>], qkv[<span class="hljs-number">1</span>], qkv[<span class="hljs-number">2</span>]  <span class="hljs-comment"># make torchscript happy (cannot use tensor as tuple)</span><br><br>        <span class="hljs-comment"># transpose: -&gt; [batch_size, num_heads, embed_dim_per_head, num_patches + 1]</span><br>        <span class="hljs-comment"># @: multiply -&gt; [batch_size, num_heads, num_patches + 1, num_patches + 1] 集体做矩阵乘</span><br>        attn = (q @ k.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) * self.scale<br>        attn = attn.softmax(dim=-<span class="hljs-number">1</span>) <span class="hljs-comment"># 对行向量做softmax</span><br>        attn = self.attn_drop(attn)<br><br>        <span class="hljs-comment"># @: multiply -&gt; [batch_size, num_heads, num_patches + 1, embed_dim_per_head] </span><br>        <span class="hljs-comment"># transpose: -&gt; [batch_size, num_patches + 1, num_heads, embed_dim_per_head]</span><br>        <span class="hljs-comment"># reshape: -&gt; [batch_size, num_patches + 1, total_embed_dim]</span><br>        x = (attn @ v).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).reshape(B, N, C)<br>        x = self.proj(x) <span class="hljs-comment"># 相当于拼接好后的Z矩阵与W^O矩阵相乘</span><br>        x = self.proj_drop(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<div class="note note-info">
            <p>nn.Linear()就是全连接(如<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.924661em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span>,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>是输入参数,可以是多维的,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span></span></span></span>是权重矩阵,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span>是偏置向量),对x.shape的最后一个维度最连接操作,也可以将其理解为矩阵乘,其参数由训练迭代更新(源码中将其加入了nn.Parameter()中)<br />nn.Droupout()防止过拟合,参数表示不被激活的神经元的占比<br />@-&gt;矩阵乘 *-&gt;矩阵点乘(又或者说逐向量内积)</p>
          </div>
<h4 id="mlp-block"><a class="markdownIt-Anchor" href="#mlp-block"></a> MLP block</h4>
<p>从上方ViT Encoder Block的图中可以看出,MLP是由两层全连接层和GELU激活函数构成,第一层将其维度变为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>768</mn><mi>x</mi><mn>4</mn></mrow><annotation encoding="application/x-tex">768x4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">7</span><span class="mord">6</span><span class="mord">8</span><span class="mord mathnormal">x</span><span class="mord">4</span></span></span></span>,第二层将其维度变回768(<mark>没搞明白为啥这样变</mark>).代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Mlp</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_features, hidden_features=<span class="hljs-literal">None</span>, out_features=<span class="hljs-literal">None</span>, act_layer=nn.GELU, drop=<span class="hljs-number">0.</span></span>):</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        out_features = out_features <span class="hljs-keyword">or</span> in_features<br>        hidden_features = hidden_features <span class="hljs-keyword">or</span> in_features<br>        self.fc1 = nn.Linear(in_features, hidden_features) <span class="hljs-comment"># hidden_features == 4*in_features</span><br>        self.act = act_layer()<br>        self.fc2 = nn.Linear(hidden_features, out_features) <span class="hljs-comment"># out_features == in_features</span><br>        self.drop = nn.Dropout(drop)<br><br>		<span class="hljs-comment"># 按照上述流程图的顺序执行的</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        x = self.fc1(x)<br>        x = self.act(x)<br>        x = self.drop(x)<br>        x = self.fc2(x)<br>        x = self.drop(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<h3 id="mlp-head"><a class="markdownIt-Anchor" href="#mlp-head"></a> MLP HEAD</h3>
<p>切片,取第0行的向量(CLS),然后做全连接,流程如下图所示<br />
<img src="http://qny.ayyha.store/MLP-HEAD.png" srcset="/img/loading.gif" lazyload alt="MLP-HEAD" /><br />
关于上面的Pre-Logits,在ImageNet-1K无需用到,直接设置为None;在ImageNet-21K里用到了,就是一个全连接层+<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">n</span><span class="mord mathnormal">h</span></span></span></span>激活函数</p>
<h2 id="vit执行流程"><a class="markdownIt-Anchor" href="#vit执行流程"></a> ViT执行流程</h2>
<p>详见下图,图片源于B站UP主:霹雳吧啦Wz<br />
<img src="http://qny.ayyha.store/ViT%20Execution%20Process.png" srcset="/img/loading.gif" lazyload alt="ViT Execution Process" /></p>
<h2 id="vit实验"><a class="markdownIt-Anchor" href="#vit实验"></a> ViT实验</h2>
<p>拿的别人的模型跑的,但是好像用CUDA的地方出了点问题,在调BUG,跑好了会摆上来</p>
<h2 id="vit与传统cnn的差异"><a class="markdownIt-Anchor" href="#vit与传统cnn的差异"></a> ViT与传统CNN的差异</h2>
<p>关于二者的差异在<strong>Do Vision Transformers See Like Convolutional Neural Networks</strong>这篇论文中进行了详细的比对.下面将对其结果和主要使用分析工具进行介绍</p>
<p>原论文连接:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.08810.pdf">Do Vision Transformers See Like Convolutional Neural Networks?</a><br />
<mark>非常粗粒度的看了这篇论文</mark></p>
<h3 id="主要分析工具"><a class="markdownIt-Anchor" href="#主要分析工具"></a> 主要分析工具</h3>
<p>CKA(Centered Kernel Alignment),可用于<strong>计算神经网络表征(neural network representation)的相似度</strong>.可以用于计算<strong>同一模型不同层间的表征相似度</strong>,或者是<strong>不同模型间的层的表征相似度</strong>,具体计算公式不细说(<mark>主要是我没弄明白</mark>),知其定义即可理解该论文对ViT和ResNet间的差异性分析.</p>
<h3 id="vit和resnet的比较"><a class="markdownIt-Anchor" href="#vit和resnet的比较"></a> ViT和ResNet的比较</h3>
<h4 id="同一模型间的不同层级的表征相似性比较"><a class="markdownIt-Anchor" href="#同一模型间的不同层级的表征相似性比较"></a> 同一模型间的不同层级的表征相似性比较</h4>
<p><img src="http://qny.ayyha.store/representation%20structure%20of%20ViTs%20and%20CNN.png" srcset="/img/loading.gif" lazyload alt="representation strcuture of ViTs and CNN" /><br />
从上面这个图可以很清晰的看出来:<strong>ViT模型整体具有高度的表征相似性,而ResNet只是局部具有较高的表征相似性,如低层和高层,且lower layers和higher layers相似性很小</strong></p>
<h4 id="不同模型间的对应层级的表征相似性比较"><a class="markdownIt-Anchor" href="#不同模型间的对应层级的表征相似性比较"></a> 不同模型间的对应层级的表征相似性比较</h4>
<p><img src="http://qny.ayyha.store/representation%20structure%20between%20ViTs%20and%20CNN.png" srcset="/img/loading.gif" lazyload alt="representation structure between ViTs and CNN" /><br />
从上面这个heatmap图可以看出来:ResNet中较多的较低层与ViT中较少的较低层具有相似性,但是在高层间(最高的那一片区域)二者相似性很低</p>
<p>以上对模型自身及模型间的层与层的表征相似性的说明了:</p>
<ul>
<li><strong>ViT最高层对于ResNet来说,具有相当不同的表示</strong></li>
<li><strong>ViT在较低层和较高层间的传播表示更为强烈</strong></li>
<li><strong>ViT计算较低层表征的方式与ResNet较低层的不同</strong></li>
</ul>
<h4 id="层表征中的局部和全局信息"><a class="markdownIt-Anchor" href="#层表征中的局部和全局信息"></a> 层表征中的局部和全局信息</h4>
<p>这个标题主要的意思就是,不同模型中<strong>低层和高层中学习到了的是什么样的信息,局部的或是全局的或是二者皆有</strong><br />
因此这里的层表征主要指代的就是<strong>低层</strong>和<strong>高层</strong>的,于ViT而言就是最开始的encoder block和最后一次的encoder block<br />
之于CNN而言,我们知道,它卷积其实受限于相邻的区域,即使stride有所调整,也是局部的(于低层而言,<mark>我觉得高层也是,某一块kernel对应的是一只猫的耳朵或是尾巴,其实也是局部信息</mark>)<br />
以下是ViT的低层和高层部分的学习到的信息的图(注意,这里的Mean Distance是一种用单个头的注意力权重,就是之前说的Z<sub>i</sub>(i∈[0,15])来加权pixel distance(<mark>我觉得是对应patch的dimension</mark>),然后做平均得到的结果.大距离说明是全局信息,小距离说明是局部信息)<br />
<img src="http://qny.ayyha.store/Local%20and%20Global%20information%20that%20ViT%20learned.png" srcset="/img/loading.gif" lazyload alt="Local and Global information that ViT learned" /><br />
显而易见,<strong>encoder block在低层时,学到的既有局部信息也有全局信息,而高层的encoder block学习到的都是全局信息</strong>(<mark>个人认为就是自注意力机制脱离了邻域关注的问题,使得低层次也可以学习到全局的信息</mark>)</p>
<p>当然,这只是说明ViT跟CNN学习方式不同,并非说局部信息不好的意思.论文里作者也用了没有pretrain的ViT,其效果很烂之余,也发现其压根没学到啥局部信息,反而印证了前期学习中局部信息的重要性,效果图如下:<br />
<img src="http://qny.ayyha.store/Local%20and%20Global%20information%20which%20no%20pretrained%20ViT%20learned.png" srcset="/img/loading.gif" lazyload alt="Local and Global information which no pretrained ViT learned" /></p>
<p>那么全局信息有啥用呢,作者通过对encoder block 1和2里面各自的16个head划分成多个子集,子集范围对应着[多数含局部信息的heads,多数含全局信息的heads],用这些子集和低层的ResNet计算CKA,得出下图:<br />
<img src="http://qny.ayyha.store/Local%20and%20Global%20ViTs%20Representations%20compared%20to%20ResNets.png" srcset="/img/loading.gif" lazyload alt="Local and Global ViTs Representations compared to ResNets" /><br />
结果显而易见,随着全局信息增多(即平均距离增大),基本上二者CKA单调递减<br />
<mark>作者其实没明确给出ViT中encoder block低层且head较小时学到的全局信息有啥用,但我觉得这个局部信息(基于Mean Distance这种度量方式)可能正是源于我们多头机制想要规避开的<strong>对于自身所在词的过度关注</strong>,而随着后面cocat成一个完整的Z与W<sup>O</sup>做乘积时,其局部性被削弱(这也跟Multi-Head Attention这个机制有关)</mark><br />
<mark>而此刻输出值作为下一个encoder block的输入,其包含了较多的全局信息,这使得<strong>我们较低层与较高层构建出一定的相似性</strong>,当然这也与skip connection有关(其实这里说的就是skip connection)</mark></p>
<p>接着作者还对<strong>有效感受野(ERF)<strong>进行了分析,如下图示:<br />
<img src="http://qny.ayyha.store/ViT%20and%20ResNet%20ERF.png" srcset="/img/loading.gif" lazyload alt="ViT and ResNet ERF" /><br />
我们知道,卷积的有效感受野受</strong>kernel大小</strong>以及<strong>下采样层</strong>影响,因此一开始很小;而自注意力机制使得ViT的有效感受野不受局部信息局限,还多了全局信息,因此有效感受野比较大;<br />
而之后ResNet的ERF以局部扩增的方式增大(<strong>高度局部化</strong>),而ViT的ERF则是从局部转向全局,且高度依赖于中心的patch,这与skip connection有强烈关系!下图是pre-residual的感受野:<br />
<img src="http://qny.ayyha.store/Pre-residual%20receptive%20fields%20of%20all%20ViT-B32%20sublayers.png" srcset="/img/loading.gif" lazyload alt="Pre-residual receptive fields of all ViT-B/32 sublayers" /><br />
可以看出上图(比较Attention12),可以看出残差连接制约着感受野对于中心patch的依赖性</p>
<h4 id="skip-connection在vit中发挥的作用"><a class="markdownIt-Anchor" href="#skip-connection在vit中发挥的作用"></a> skip connection在ViT中发挥的作用</h4>
<p>根据先前的ViT不同层级做CKA进行相似性比较的图,我们知道了它的表征具有<strong>高度一致性</strong>,这是由我们这里要讨论的skip connection 发挥的作用<br />
我们通过范数比:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∥</mi><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">∥</mi></mrow><mrow><mi mathvariant="normal">∥</mi><mi>f</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\Vert z_i \Vert}{\Vert f(z_i)\Vert}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.04398em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mord mtight">∥</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.04398em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight">∥</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>来进行探讨,其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">∥</mi></mrow><annotation encoding="application/x-tex">\Vert z_i \Vert</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∥</span></span></span></span>是来自于skip connection的第i个层的hidden representation,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><mi>f</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi></mrow><annotation encoding="application/x-tex">\Vert f(z_i) \Vert</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∥</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∥</span></span></span></span>是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>经过long branch后的值,这里的long branch指的是MLP或是self-attention</p>
<p>至此,我们知道了,<strong>若是范数比比值大则意味着skip connection起主要作用,若是范数比比值小则意味着long branch起主要作用</strong></p>
<p>以下是根据范数比所作的heatmap,需注意CLS token和别的spatial token是分开来讨论的:<br />
<img src="http://qny.ayyha.store/ratio%20of%20norms.png" srcset="/img/loading.gif" lazyload alt="ratio of norms" /></p>
<p>根据左图,显而易见,CLS token(token[0])和别的spatial token的受影响方式恰好相反<br />
<strong>CLS token是网络前期(Block index小的部分)范数比大,即受skip connection影响大,而网络后期则是受long branch影响大,spatial token则相反</strong></p>
<p>根据右图,除了彰显了上述结论,也可以看出<strong>ViT较ResNet受skip connection影响更大</strong></p>
<p>作者又做了个干预性实验来证明skip connection对ViT表征结构高度一致性的影响,即移除中间某一个block的skip connection,图示如下:<br />
<img src="http://qny.ayyha.store/ViT%20remove%20a%20block%27s%20skip%20connection.png" srcset="/img/loading.gif" lazyload alt="ViT remove a block's skip connection" /><br />
可见,若是移除了某一个block的skip connection,那在该block前后的层的表征相似性则非常低.由此<strong>佐证了skip connection对ViT层间表征相似性的作用</strong>!</p>
<h4 id="vit在higher-layers的空间位置信息是否仍然保留"><a class="markdownIt-Anchor" href="#vit在higher-layers的空间位置信息是否仍然保留"></a> ViT在higher layers的空间位置信息是否仍然保留</h4>
<p>知道了前面ViT与ResNet的一些差别后,还想知道它的空间信息在较高的层是否仍然保留,这对transformer是否可以干除了图像分类之外的事很重要,如目标检测</p>
<p>我们通过对最后一个block的token与最开始输入的patch token进行比较(计算不同位置的CKA值),然后做heatmap,可以看出它们的相似性,即空间位置信息是否被high layers保留.图示如下:<br />
<img src="http://qny.ayyha.store/spatial%20location%20of%20ViT%20%26%20ResNet.png" srcset="/img/loading.gif" lazyload alt="spatial location of ViT &amp; ResNet" /></p>
<p>显然,ViT的空间位置信息被保留下来了,而且所选的单个token与最开始的对应的patch相似性最强,而边缘部分的token也是如此,但其与其他边缘位置相似性也很高.可以看出ViT对空间位置信息有保留!相较之下,ResNet则体现不出来,按作者的说法就是significantly weaker的位置信息</p>
<p>然后作者还对ResNet为啥会位置信息保留得如此薄弱进行了实验,认为是分类所采用的方法导致的,ViT采用的是一个单独的token-&gt;CLS token,对原位置信息本就不影响,而ResNet在训练时分类用的是全局平均池化(GAP),把信息都杂糅在一起了,哪里还有原来规整的位置信息</p>
<p>因此,就把ViT里面的CLS token去掉,通过GAP来做分类,结果说明了确实是GAP的原因,图示如下:<br />
<img src="http://qny.ayyha.store/ViT%20use%20GAP%20to%20classification.png" srcset="/img/loading.gif" lazyload alt="ViT use GAP to classification" /></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/transformer/">transformer</a>
                    
                      <a class="hover-with-bg" href="/tags/cv/">cv</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/10/06/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95%E5%90%88%E9%9B%86/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">图像增强方法合集</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/10/02/transformer%E7%90%86%E8%A7%A3/">
                        <span class="hidden-mobile">transformer理解</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"1db059241d978c80eecd","clientSecret":"fa4f54290b2645c1dd4af84d9241dcb8c3e8e637","repo":"ayyBlog","owner":"ayyha","admin":["ayyha"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: '9e228ccef292c8d0379cd69c679385d2'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- KaTeX -->
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css" />
  








  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?9c7ed39aa5906acb06d9f9cb7df236ae";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300,"hOffset":30,"vOffset":-50,"superSample":2},"mobile":{"show":false},"react":{"opacityDefault":0.7,"opacityOnHover":0.2}});</script></body>
</html>
