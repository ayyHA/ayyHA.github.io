

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="pytorch知识学习">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <meta name="description" content="pytorch知识学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Some knowledge of Pytorch">
<meta property="og:url" content="http://example.com/2023/03/14/Some-knowledge-of-Pytorch/index.html">
<meta property="og:site_name" content="ayyHA&#39;s blog">
<meta property="og:description" content="pytorch知识学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/DiRezlK3bdN8BAp.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/57SYoj3rz4Lmftv.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/DFwYUoTCq4em1uP.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/dG2D8SmkpXiHUAJ.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/iw6r3x1fDpNsymY.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/n8zqxsmtRYagSuG.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/mZ2ADBWYPflF31V.png">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/lFW9cqwrdLek2xS.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/11/03/nvNOErxZqjhtWbk.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/DNfQgtswnzu9UPe.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/04/FvhKZHnsbom3SeG.jpg">
<meta property="og:image" content="https://s2.loli.net/2023/12/05/Bpr61KqoZsGUI5m.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/10/vFOdzJ5gT4Pt2eY.png">
<meta property="article:published_time" content="2023-03-13T16:25:38.000Z">
<meta property="article:modified_time" content="2024-01-20T08:30:56.020Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s2.loli.net/2023/11/03/DiRezlK3bdN8BAp.jpg">
  
  <title>Some knowledge of Pytorch - ayyHA&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/mouse.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"9c7ed39aa5906acb06d9f9cb7df236ae","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ayyHA</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Some knowledge of Pytorch">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2023-03-14 00:25" pubdate>
        2023年3月14日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      18k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      56 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Some knowledge of Pytorch</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：3 个月前
                
              </p>
            
            <div class="markdown-body">
              <h2 id="torch中涉及到的一些函数记录"><a class="markdownIt-Anchor" href="#torch中涉及到的一些函数记录"></a> torch中涉及到的一些函数记录</h2>
<h2 id="torchnnmodule中的modules和children的区别"><a class="markdownIt-Anchor" href="#torchnnmodule中的modules和children的区别"></a> torch.nn.Module中的modules()和children()的区别</h2>
<p>首先构建一个全连接网络,看看它的结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_dim, hidden_dim1, hidden_dim2, hidden_dim3, out_dim</span>):</span><br>        <span class="hljs-built_in">super</span>(TestNet, self).__init__()<br>        self.layer1 = nn.Sequential(<br>            nn.Linear(in_dim, hidden_dim1),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        self.layer2 = nn.Sequential(<br>            nn.Linear(hidden_dim1, hidden_dim2),<br>            nn.Sigmoid()<br>        )<br>        self.layer3 = nn.Sequential(<br>            nn.Linear(hidden_dim2, hidden_dim3),<br>            nn.Tanh()<br>        )<br>        self.layer4 = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.layer4(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    net = TestNet(<span class="hljs-number">5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># net arch</span><br>    <span class="hljs-built_in">print</span>(net)<br></code></pre></td></tr></table></figure>
<p>其结构为:</p>
<p><img src="https://s2.loli.net/2023/11/03/DiRezlK3bdN8BAp.jpg" srcset="/img/loading.gif" lazyload alt="fcn arch" /></p>
<p>其结构图大致如下(Sequential以layer的标识号区别之,别的亦同)</p>
<pre class="mermaid">graph
TestNet-->Sequential1 & Sequential2 & Sequential3 & ReLU
Sequential1 --> Linear1 & ReLU1
Sequential2 --> Linear2 & Sigmoid
Sequential3 --> Linear3 & Tanh
%% layer4 --> ReLU4</pre>
<p>而当我们利用<code>net.children()</code>打印时,发现其是generator类型,也即是iterator类型,因此可以通过循环将其输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    net = TestNet(<span class="hljs-number">5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># children arch</span><br>    <span class="hljs-keyword">for</span> i, ele <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(net.children()):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i, ele))<br></code></pre></td></tr></table></figure>
<p>其输出结果如下:</p>
<p><img src="https://s2.loli.net/2023/11/03/57SYoj3rz4Lmftv.jpg" srcset="/img/loading.gif" lazyload alt="fcn-children-arch" /></p>
<p>可见<strong>通过children()获得的结构仅包含最外一层</strong>,也即可以通过如下方式获得其最外层:<code>print(list(net.children())[0])</code>,即可以获得第0个Sequential:</p>
<p><img src="https://s2.loli.net/2023/11/03/DFwYUoTCq4em1uP.jpg" srcset="/img/loading.gif" lazyload alt="fcn-children-arch-0" /></p>
<p>而通过modules()获得的也是generator类型,因此也用循环将其输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    net = TestNet(<span class="hljs-number">5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># modules arch</span><br>    <span class="hljs-keyword">for</span> i,ele <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(net.modules()):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i,ele))<br></code></pre></td></tr></table></figure>
<p>其输出结果为:</p>
<p><img src="https://s2.loli.net/2023/11/03/dG2D8SmkpXiHUAJ.png" srcset="/img/loading.gif" lazyload alt="fcn-modules-arch" /></p>
<p>可见,其结果类似于深搜,直接把整个结构DFS了一遍</p>
<p>因此可得<code>children()</code>和<code>modules()</code>的区别如下:</p>
<ul>
<li>通过<code>children()</code>获取网络层级结构,只会取最外层,即根节点下的一层</li>
<li>通过<code>modules()</code>获取网络层级结构,则类似对网络结构进行DFS,依次输出</li>
</ul>
<div class="note note-info">
            <p>关于<code>named_children()</code>和<code>named_modules()</code>就不再赘述,因为是在<code>children()</code>和<code>modules()</code>的基础上加了个名字,同样的,它们也是generator类型,可以通过循环遍历,不过对应的是<code>name</code>和<code>module</code>(就是上面代码中的<code>ele</code>),可以通过列表推导式查看相应的结果:</p><p><code>net_named_children = [x for x in net.named_children()]</code></p><p><code>net_named_modules = [x for x in net.named_modules()]</code></p><p>而<code>net.parameters()</code>和<code>net.named_parameters()</code>打印的是模型每层的参数,而多了个<code>named</code>的方法则是把对应的层/子模块的名称也带上了.可以通过列表推导式来查看相应的结果.通过它们的类型也是generator.</p><p><code>net_parameters = [x for x in net.parameters()]</code></p><p><code>net_named_parameters =[x for x in net.named_parameters()]</code></p>
          </div>
<h2 id="pytorch中的模型容器"><a class="markdownIt-Anchor" href="#pytorch中的模型容器"></a> pytorch中的模型容器</h2>
<p>利用模型容器,可以<strong>自动的将module注册到网络上</strong>,以及<strong>将module的parameters添加到网络上</strong></p>
<h3 id="nnsequential"><a class="markdownIt-Anchor" href="#nnsequential"></a> nn.Sequential</h3>
<p>将如<code>Conv2d</code>,<code>BatchNorm2d</code>,<code>ReLU</code>等的module放入<code>nn.Sequential</code>容器中,将会<strong>按照放置的顺序执行</strong>,e.g.:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br>layer1 = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>),<br>    nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>    nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>),<br>    nn.BatchNorm2d(<span class="hljs-number">512</span>),<br>    nn.ReLU()<br>)<br><span class="hljs-built_in">print</span>(layer1)<br></code></pre></td></tr></table></figure>
<p>其网络层级结构如下所示</p>
<p><img src="https://s2.loli.net/2023/11/03/iw6r3x1fDpNsymY.png" srcset="/img/loading.gif" lazyload alt="sequential-arch" /></p>
<p>其还可以用<code>OrderedDict</code>去为每个module命名,e.g.:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> collections<br><br>layer1 = nn.Sequential(collections.OrderedDict([<br>    (<span class="hljs-string">&quot;conv1&quot;</span>, nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)),<br>    (<span class="hljs-string">&quot;bn1&quot;</span>, nn.BatchNorm2d(<span class="hljs-number">256</span>)),<br>    (<span class="hljs-string">&quot;relu1&quot;</span>, nn.ReLU()),<br>    (<span class="hljs-string">&quot;conv2&quot;</span>, nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)),<br>    (<span class="hljs-string">&quot;bn2&quot;</span>, nn.BatchNorm2d(<span class="hljs-number">512</span>)),<br>    (<span class="hljs-string">&quot;relu2&quot;</span>, nn.ReLU())<br>]))<br><span class="hljs-built_in">print</span>(layer1)<br></code></pre></td></tr></table></figure>
<p>网络层级结构如下示</p>
<p><img src="https://s2.loli.net/2023/11/03/n8zqxsmtRYagSuG.png" srcset="/img/loading.gif" lazyload alt="sequential-arch-orderedDict" /></p>
<h3 id="nnmodulelist"><a class="markdownIt-Anchor" href="#nnmodulelist"></a> nn.ModuleList</h3>
<p>该模型容器类似于list,较之<code>nn.Sequential</code>可以更灵活的使用,充当了存放module的容器,执行的顺序在<code>forward</code>中自行定义,较之原生的list,则是可以注册module于网络以及对其参数添加进<code>nn.Parameters()</code>中.e.g.:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(TestNet, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        self.bn1 = nn.BatchNorm2d(<span class="hljs-number">256</span>)<br>        self.relu1 = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.layer1 = [nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">100</span>), nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-number">10</span>)]<br>        self.layer2_ = [nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">200</span>), nn.Linear(<span class="hljs-number">200</span>, <span class="hljs-number">10</span>)]<br>        self.layer2 = nn.ModuleList(self.layer2_)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        x = self.conv1(x)<br>        x = self.bn1(x)<br>        x = self.relu1(x)<br>        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> self.layer1:<br>            x = l(x)<br>        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> self.layer2:<br>            x = l(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<p>其执行结果表示,用list充当容器,并不能将所需的module注册,而ModuleList则可以,结果如下图示</p>
<p><img src="https://s2.loli.net/2023/11/03/mZ2ADBWYPflF31V.png" srcset="/img/loading.gif" lazyload alt="modulelist-arch" /></p>
<h2 id="datasetsamplerdataloader三者关系"><a class="markdownIt-Anchor" href="#datasetsamplerdataloader三者关系"></a> Dataset,Sampler,DataLoader三者关系</h2>
<p>当我们需要对数据文件进行解析获得数据或者自己创建的数据集<code>ImageFolder</code>不能够满足我们的要求,我们则需要自定义一个<code>Dataset</code>类</p>
<p>而定义好一个<code>Dataset</code>类后,我们可以通过循环或索引得到对应的一条数据,形如<code>data,target</code>,而<code>Sampler</code>则是对这些一条条的数据进行采样的工具,Pytorch提供的主要有<code>SequentialSampler</code>和<code>RandomSampler</code>,这些采样器采样得到的都是这些数据的索引</p>
<p>显然我们一条条读取数据并不能满足我们的需求,我们更期望的是以batch为单位的读取数据,因而有<code>BatchSampler</code>这么一个批采样器,它会对我们Sampler,如<code>SequentialSampler</code>采样得到的一个个索引整理成一个<code>batch_size</code>大小的索引序列</p>
<p>在DataLoader里面,我们便是对<code>BatchSampler</code>这个批采样器采样得到的索引序列进行处理,通过传入的<code>dataset</code>参数读取一条条数据,整理成一个<code>List[Tuple[Tensor,Tensor]]</code>这么一个<code>batch_list</code>的形式,交由<code>collate_fn</code>对这一个<code>batch_size</code>大小的数据进行整理,而后得到我们在循环里对<code>DataLoader</code>遍历的数据</p>
<p>以下展现的是工作进程(num_worker)为0的<code>DataLoader</code>处理方式(跟我上面说的流程一样):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># dataloader.py:</span><br> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DataLoader</span>(<span class="hljs-params"><span class="hljs-type">Generic</span>[T_co]</span>):</span><br> <span class="hljs-comment"># ...</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_iterator</span>(<span class="hljs-params">self</span>) -&gt; &#x27;_BaseDataLoaderIter&#x27;:</span><br>        <span class="hljs-keyword">if</span> self.num_workers == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> _SingleProcessDataLoaderIter(self)<br> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_SingleProcessDataLoaderIter</span>(<span class="hljs-params">_BaseDataLoaderIter</span>):</span><br> <span class="hljs-comment"># ...</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_next_data</span>(<span class="hljs-params">self</span>):</span><br>        index = self._next_index()  <span class="hljs-comment"># may raise StopIteration</span><br>        data = self._dataset_fetcher.fetch(index)  <span class="hljs-comment"># may raise StopIteration</span><br>        <span class="hljs-keyword">if</span> self._pin_memory:<br>            data = _utils.pin_memory.pin_memory(data)<br>        <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># fetch.py</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_MapDatasetFetcher</span>(<span class="hljs-params">_BaseDatasetFetcher</span>):</span><br> <span class="hljs-comment"># ...</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch</span>(<span class="hljs-params">self, possibly_batched_index</span>):</span><br>        <span class="hljs-keyword">if</span> self.auto_collation:<br>            data = [self.dataset[idx] <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> possibly_batched_index]<br>        <span class="hljs-keyword">else</span>:<br>            data = self.dataset[possibly_batched_index]<br>        <span class="hljs-keyword">return</span> self.collate_fn(data)<br></code></pre></td></tr></table></figure>
<p>自定义一个<code>Dataset</code>类需要的工作:</p>
<ul>
<li>继承<code>torch.utils.data.Dataset</code></li>
<li>在<code>__init__</code>中传入需要处理的数据(可能是数据的目录啥的),对数据的预处理方法</li>
<li>在**<code>__getitem__</code>**中完成对数据的解析,预处理,然后返回对应的数据,如<code>return data,target</code></li>
<li>在<code>__len__</code>中反应需要处理的数据集的大小</li>
</ul>
<p>自定义一个Sampler类需要的工作:</p>
<ul>
<li>继承<code>torch.utils.data.Sampler</code></li>
<li>在<code>__iter__</code>方法中返回一个iterator</li>
</ul>
<h2 id="torchutilsdatadataloader中的collate_fn"><a class="markdownIt-Anchor" href="#torchutilsdatadataloader中的collate_fn"></a> torch.utils.data.DataLoader中的collate_fn</h2>
<p>在涉及到数据集处理的时候,我们需要考虑到<code>torch.utils.data.Dataset</code>类,以及<code>torch.utils.data.DataLoader</code>类</p>
<p>其中前者是我们对数据集的处理,比如解析数据集数据,然后<strong>重写<code>__len__()</code>以及<code>__getitem__()</code>方法</strong>,而后在其中对数据进行<strong>预处理</strong>操作,然后才会得到**<code>data,target</code><strong>这样的数据,其中<code>data</code>可以是图像之类的,而<code>target</code>则可能是标签,GTbox的位置等信息,这时我们通过<code>__getitem__</code>方法</strong>只是获取到一条<code>data,target</code>数据**,处理批量数据的任务则由<code>DataLoader</code>承担</p>
<p>我们在不考虑这一部分将要提及的<code>collate_fn</code>方法前,看一看<code>DataLoader</code>是怎么样将一条数据变成一个<code>batch_size</code>的数据的</p>
<p>以下是代码部分:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data<br>data = torch.tensor([<br>    [<span class="hljs-number">0.4698</span>, <span class="hljs-number">0.6971</span>, <span class="hljs-number">0.9499</span>, <span class="hljs-number">0.3641</span>],<br>    [<span class="hljs-number">0.0896</span>, <span class="hljs-number">0.5345</span>, <span class="hljs-number">0.5603</span>, <span class="hljs-number">0.5409</span>],<br>    [<span class="hljs-number">0.4988</span>, <span class="hljs-number">0.2155</span>, <span class="hljs-number">0.1244</span>, <span class="hljs-number">0.3456</span>],<br>    [<span class="hljs-number">0.4812</span>, <span class="hljs-number">0.0108</span>, <span class="hljs-number">0.1885</span>, <span class="hljs-number">0.8593</span>],<br>    [<span class="hljs-number">0.6564</span>, <span class="hljs-number">0.3428</span>, <span class="hljs-number">0.8815</span>, <span class="hljs-number">0.3558</span>]])<br>target = torch.tensor([<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>])<br>dataset = Data.TensorDataset(data, target)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dataset:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i&#125;</span>&quot;</span>)<br><br>batch_size = <span class="hljs-number">2</span><br>dataloader = Data.DataLoader(batch_size=batch_size, dataset=dataset)<br><br><span class="hljs-keyword">for</span> d, t <span class="hljs-keyword">in</span> dataloader:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;data:<span class="hljs-subst">&#123;d&#125;</span>\ntarget:<span class="hljs-subst">&#123;t&#125;</span>\n&quot;</span>)<br><span class="hljs-comment"># 输出结果:</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([0.4698, 0.6971, 0.9499, 0.3641]), tensor(4))</span><br><span class="hljs-string">(tensor([0.0896, 0.5345, 0.5603, 0.5409]), tensor(4))</span><br><span class="hljs-string">(tensor([0.4988, 0.2155, 0.1244, 0.3456]), tensor(1))</span><br><span class="hljs-string">(tensor([0.4812, 0.0108, 0.1885, 0.8593]), tensor(3))</span><br><span class="hljs-string">(tensor([0.6564, 0.3428, 0.8815, 0.3558]), tensor(1))</span><br><span class="hljs-string">data:tensor([[0.4698, 0.6971, 0.9499, 0.3641],</span><br><span class="hljs-string">        [0.0896, 0.5345, 0.5603, 0.5409]])</span><br><span class="hljs-string">target:tensor([4, 4])</span><br><span class="hljs-string"></span><br><span class="hljs-string">data:tensor([[0.4988, 0.2155, 0.1244, 0.3456],</span><br><span class="hljs-string">        [0.4812, 0.0108, 0.1885, 0.8593]])</span><br><span class="hljs-string">target:tensor([1, 3])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>
<p>可见,<code>dataset</code>的结构应是一个<code>Sequential[Tuple[Tensor,Tensor]]</code>,而其中一条数据中的tuple里面则包含了<code>data</code>以及<code>target</code>,也即是说通过<code>Dataset</code>我们逐条获取数据得到的是形如<code>(data,target)</code>这样的数据</p>
<p>而显然我们更加希望<code>data</code>归为<code>data</code>,而<code>target</code>归为<code>target</code>,就如用普通的<code>DataLoader</code>得到的结果一样,输出的结果是一个batch的data和一个batch的target</p>
<p>显然<code>DataLoader</code>在其内部即帮我们完成了①按batch_size划分数据;②<code>data</code>为<code>data</code>,<code>target</code>为<code>target</code></p>
<p>以下,我们采用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y=x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>的形式于<code>collate_fn</code>看下输出的结果是什么</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">loader = Data.DataLoader(batch_size=batch_size, dataset=dataset, collate_fn=<span class="hljs-keyword">lambda</span> x: x)<br>it = <span class="hljs-built_in">iter</span>(loader)<br>batch_data = <span class="hljs-built_in">next</span>(it)<br><span class="hljs-built_in">print</span>(batch_data)<br><span class="hljs-comment"># 输出结果</span><br><span class="hljs-comment"># [(tensor([0.4698, 0.6971, 0.9499, 0.3641]), tensor(4)), (tensor([0.0896, 0.5345, 0.5603, 0.5409]), tensor(4))]</span><br></code></pre></td></tr></table></figure>
<p>根据输出结果不难看出: 在进入<code>collate_fn</code>之前,数据已经按<code>batch_size</code>划分好了,其结构为:<code>List[Tuple(Tensor,Tensor)]</code>,其中List的大小是batch_size的大小,但是数据格式依旧是<code>dataset</code>的结构,因而<code>collate_fn</code>这个方法是用来<strong>调整数据格式</strong>的,在我们不调用自定义的<code>collate_fn</code>时,会用系统默认的函数,将输出调整为batch_size大小的data和target这两个部分</p>
<p>以下是等价于系统默认的<code>collate_fn</code>(能将输出划分为data和target两部分):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># input x: List[Tuple[Tensor,Tensor],...]</span><br>collate_func = <span class="hljs-keyword">lambda</span> x:(<br>	torch.cat(<br>        <span class="hljs-comment"># data: [4] -&gt; [1,4] -&gt; [N,4]</span><br>        <span class="hljs-comment"># target: [1] -&gt; [1,1] -&gt; [N,1]</span><br>        [x[i][j].unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x))],<span class="hljs-number">0</span><br>    ) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x[<span class="hljs-number">0</span>]))<br>)<br></code></pre></td></tr></table></figure>
<p>以上是分别对data,target数据进行取出,然后扩充维度,而后对扩充的维度进行拼接,便得到了期望的结果</p>
<p>一般来说,不会用到它,但<mark>我在网上以及个人思考后</mark>(pytorch这方面源码没看懂😂),应该是通过<code>torch.stack()</code>进行的维度堆叠,因而如果图片的尺寸或者target中如标签的数目不等,则需要自定义</p>
<p>比如说一个batch_size是2,那两张图片分别是有两个和三个目标,即对应的target为<code>[2,5]/[3,5]</code>无法用<code>stack()</code>对它们简单的拼接,因而需要自己定义一个<code>collate_fn</code>去处理这个问题,自己去定义一个batch出来的数据的格式</p>
<h2 id="torch数据并行"><a class="markdownIt-Anchor" href="#torch数据并行"></a> torch数据并行</h2>
<p>以下内容参考文章:</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/648596428">分布式训练</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/467103734">快速上手</a></p>
<h3 id="torchnndataparallel"><a class="markdownIt-Anchor" href="#torchnndataparallel"></a> torch.nn.DataParallel</h3>
<h3 id="torchnndistributeddataparallel"><a class="markdownIt-Anchor" href="#torchnndistributeddataparallel"></a> torch.nn.DistributedDataParallel</h3>
<h3 id="torchdistributed"><a class="markdownIt-Anchor" href="#torchdistributed"></a> torch.distributed</h3>
<h2 id="计算图"><a class="markdownIt-Anchor" href="#计算图"></a> 计算图</h2>
<p>计算图可以表示模型中的<strong>数据</strong>经过<strong>运算</strong>后的流向，是一个有向无环图（DAG），<strong>有利于用链式法则计算梯度</strong>。它是由node和edge构成，node表示的是数据，如Tensor等，edge则表示的是计算，如加减乘除、卷积、非线性函数变换等</p>
<p><img src="https://s2.loli.net/2023/11/03/lFW9cqwrdLek2xS.jpg" srcset="/img/loading.gif" lazyload alt="ComputationGraphExample" /></p>
<p>以上的计算图中，利用代码表示并计算出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial y}{\partial w}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.277216em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>（即偏导/梯度的分量）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>w = torch.tensor([<span class="hljs-number">1.</span>],requires_grad = <span class="hljs-literal">True</span>)<br>x = torch.tensor([<span class="hljs-number">2.</span>],requires_grad = <span class="hljs-literal">True</span>)<br><br>a = torch.add(w,x)<br>b = torch.add(w,<span class="hljs-number">1</span>)<br>y = torch.mul(a,b)<br><br>y.backward()<br><br><span class="hljs-built_in">print</span>(w.grad)<br><br><span class="hljs-comment"># 运行结果如下，其中grad属性是取w的梯度</span><br><span class="hljs-comment"># tensor([5.])</span><br></code></pre></td></tr></table></figure>
<p>根据链式法则，我们知道，要求</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>a</mi></mrow></mfrac><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>a</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>b</mi></mrow></mfrac><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>b</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial y}{\partial w} = \frac{\partial y}{\partial a}\frac{\partial a }{\partial w} + \frac{\partial y}{\partial b}\frac{\partial b}{\partial w} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.0574399999999997em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">a</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">b</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>即是对y到w上涉及到的路径进行求和，而路径则是依次求偏导相乘得到的。从图及上式可知<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">\frac{\partial y}{\partial w} = a+b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.277216em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span>，所求得的结果与代码计算的无异</p>
<p>此外，我们知道原函数，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>w</mi><mo>+</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">a = w+x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>a</mi><mo>∗</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = a*b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.46528em;vertical-align:0em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span>，它们的不同会影响到求导过程中的计算（乘法和加法求导时的区别），因此<code>Tensor</code>中提供了<code>grad_fn</code>属性，便于获取不同运算方式时的求导规则。</p>
<p>以下是在上面代码补了几行<code>print</code>，打印出<code>grad_fn</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(a.grad_fn)<br><span class="hljs-built_in">print</span>(b.grad_fn)<br><span class="hljs-built_in">print</span>(y.grad_fn)<br><span class="hljs-comment"># 运行结果如下</span><br><span class="hljs-comment"># &lt;AddBackward0 object at 0x000002AA99CA6208&gt;</span><br><span class="hljs-comment"># &lt;AddBackward0 object at 0x000002AA99CA6208&gt;</span><br><span class="hljs-comment"># &lt;MulBackward0 object at 0x000002AA99CA6208&gt;</span><br></code></pre></td></tr></table></figure>
<p>上图给出的例子中，<code>x</code>和<code>w</code>均是输入值，是作为图中的叶子节点存在的，如果没有特别标明，在反向传播求完梯度后，非叶子节点的梯度是会被释放的，只保留叶子节点的梯度，如需保留非叶子节点的梯度，可以利用<code>retain_grad</code>属性标明</p>
<p>以下是演示代码及结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>w = torch.tensor([<span class="hljs-number">1.</span>],requires_grad = <span class="hljs-literal">True</span>)<br>x = torch.tensor([<span class="hljs-number">2.</span>],requires_grad = <span class="hljs-literal">True</span>)<br><br>a = torch.add(w,x)<br>b = torch.add(w,<span class="hljs-number">1</span>)<br>y = torch.mul(a,b)<br><br>a.retain_grad()<br>b.retain_grad()<br><br>y.backward()<br><br><span class="hljs-built_in">print</span>(w.grad)<br><span class="hljs-built_in">print</span>(a.grad_fn)<br><span class="hljs-built_in">print</span>(b.grad_fn)<br><span class="hljs-built_in">print</span>(y.grad_fn)<br><br><span class="hljs-built_in">print</span>(a.is_leaf,b.is_leaf,y.is_leaf)<br><span class="hljs-built_in">print</span>(x.is_leaf,w.is_leaf)<br><br><span class="hljs-built_in">print</span>(a.grad,b.grad,y.grad)<br><span class="hljs-comment"># 运行结果如下</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([5.])</span><br><span class="hljs-string">&lt;AddBackward0 object at 0x00000266EE747208&gt;</span><br><span class="hljs-string">&lt;AddBackward0 object at 0x00000266EE747208&gt;</span><br><span class="hljs-string">&lt;MulBackward0 object at 0x00000266EE747208&gt;</span><br><span class="hljs-string">False False False</span><br><span class="hljs-string">True True</span><br><span class="hljs-string">tensor([2.]) tensor([3.]) None</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>
<p>详细区别可见：<a target="_blank" rel="noopener" href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture08.pdf">cs231n-lecture08-pdf</a></p>
<h3 id="pytorch和tensorflow计算图比较"><a class="markdownIt-Anchor" href="#pytorch和tensorflow计算图比较"></a> PyTorch和Tensorflow计算图比较</h3>
<p>PyTorch中的计算图是动态生成的，即类似于解释型语言，它是在运行过程中动态生成的；</p>
<p>Tensorflow的计算图则是静态生成的，即类似于编译型语言，是先生成计算图，而后进行运算。</p>
<p>显然动态生成的更灵活，利于定位错误，进行debug；静态生成的则可以进行优化，更高效些。</p>
<p>PyTorch和Tensorflow现也各自都有动态和静态的计算图于各自的子库中</p>
<p><img src="https://s2.loli.net/2023/11/03/nvNOErxZqjhtWbk.png" srcset="/img/loading.gif" lazyload alt="Computation Graph Difference" /></p>
<h2 id="逐层遍历模型子模块"><a class="markdownIt-Anchor" href="#逐层遍历模型子模块"></a> 逐层遍历模型子模块</h2>
<p>当我们从<code>torchvision.models</code>中去导入一个模型的时候，比如resnet18</p>
<p><code>from torchvision.models import resnet18</code></p>
<p>我们通过<code>print(model) # resnet18</code>会打印出模型详细的层信息，这些个层信息是经由<code>OrderedDict</code>包装过的，即是通过该结构对层信息进行重命名</p>
<p>我们可以通过<code>model.__dict__</code>来查看模型的内部信息，为了便于查看，我们遍历着看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet18<br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>model = resnet18().to(device)<br><span class="hljs-built_in">print</span>(model)<br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> model.__dict__.items():<br>    <span class="hljs-built_in">print</span>(k, <span class="hljs-string">&#x27;:&#x27;</span>, v)<br></code></pre></td></tr></table></figure>
<p>以下是模型内部信息打印结果(没截全)：</p>
<p><img src="https://s2.loli.net/2023/12/04/DNfQgtswnzu9UPe.png" srcset="/img/loading.gif" lazyload alt="model-dict" /></p>
<p>不难看出，模型内部信息包含了丰富的内容，其中我们这里关注<code>_modules</code>属性，通过它我们可以很方便地对模型的层信息（子模块）逐层遍历</p>
<p>我们通过<code>model.__dict__[_modules]</code>获取模型的层信息，其中获取到的层信息是用<code>OrderedDict</code>包装过的，因此进行如下遍历：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">m_dict = model.__dict__[<span class="hljs-string">&#x27;_modules&#x27;</span>]<br><span class="hljs-keyword">for</span> name, sub_module <span class="hljs-keyword">in</span> m_dict.items():<br>    sub_module_class = sub_module.__class__<br>    sub_module_name = sub_module.__class__.__name__<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;sub_module_class:&quot;</span>, sub_module_class)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;sub_module_name:&quot;</span>, sub_module_name)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;name:%s\n&quot;</span> % name)<br></code></pre></td></tr></table></figure>
<p>以下是部分遍历结果：</p>
<p><img src="https://s2.loli.net/2023/12/04/FvhKZHnsbom3SeG.jpg" srcset="/img/loading.gif" lazyload alt="model_sub_module_iteration" /></p>
<p>通过递归可以对子模块进行完整的遍历，因为有些子模块是通过模型容器封装的，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet18<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">recursively_iter_sub_module</span>(<span class="hljs-params">module_dict, module_forward_dict</span>):</span><br>    <span class="hljs-keyword">for</span> name, sub_module <span class="hljs-keyword">in</span> module_dict.items():<br>        <span class="hljs-keyword">if</span> sub_module <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">isinstance</span>(sub_module, torch.nn.Module) <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>:<br>            <span class="hljs-keyword">break</span><br>        sub_module_class = sub_module.__class__<br>        sub_module_name = sub_module.__class__.__name__<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;sub_module_class:%s\nsub_module_name:%s\nname:%s&quot;</span> %<br>              (sub_module_class, sub_module_name, name))<br>        sub_sub_modules = sub_module.__dict__[<span class="hljs-string">&#x27;_modules&#x27;</span>]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(sub_sub_modules) == <span class="hljs-number">0</span>:<br>            module_forward_dict.update(&#123;sub_module: sub_module.forward&#125;)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(sub_sub_modules) &gt; <span class="hljs-number">0</span>:<br>            recursively_iter_sub_module(sub_sub_modules, module_forward_dict)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    model = resnet18().to(device)<br>    sub_modules = model.__dict__[<span class="hljs-string">&#x27;_modules&#x27;</span>]<br>    sub_module_forward_dict = &#123;&#125;<br>    recursively_iter_sub_module(sub_modules, sub_module_forward_dict)<br>    <span class="hljs-keyword">for</span> module, forward <span class="hljs-keyword">in</span> sub_module_forward_dict.items():<br>        <span class="hljs-built_in">print</span>(module, <span class="hljs-string">&quot;:&quot;</span>, forward)<br><br></code></pre></td></tr></table></figure>
<p>这里通过<code>&#123;module:forward&#125;</code>把所有的子模块按顺序封装了是为了后续便于对<code>forward</code>重封装</p>
<p>以下是部分运行结果图：</p>
<p><img src="https://s2.loli.net/2023/12/05/Bpr61KqoZsGUI5m.png" srcset="/img/loading.gif" lazyload alt="recursively_iter_sub_module" /></p>
<h2 id="pytorch的hook"><a class="markdownIt-Anchor" href="#pytorch的hook"></a> PyTorch的hook</h2>
<p>pytorch的hook有针对Tensor的,也有针对module的,涉及到的函数如下:</p>

<pre>
<code class="mermaid" >
graph LR
hook --> Tensor & Module
Tensor --> register_hook
Module --> register_forward_hook & register_forward_pre_hook & register_full_backward_hook & register_full_backward_pre_hook  
</code>
</pre>
<h3 id="hook-for-tensor"><a class="markdownIt-Anchor" href="#hook-for-tensor"></a> hook for Tensor</h3>
<p><code>register_hook(hook)</code>当对应<code>tensor</code>的梯度计算完的时候,这个钩子函数(即里面的hook)会被调用,<strong>该钩子函数可以用来打印中间节点的梯度信息,甚至修改计算完的梯度(虽然pytorch不建议你这么做)</strong>,这个<code>register_hook()</code>返回值是一个<code>handle</code>(<code>RemovableHandle</code>类的),这个<code>handle</code>可以调用<code>remove()</code>方法移除对应<code>tensor</code>的钩子函数</p>
<p>钩子函数的声明需要遵循如下规则:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">hook(grad) -&gt; Tensor <span class="hljs-keyword">or</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>
<p><strong>输入是这个tensor对应的梯度,返回值是一个Tensor或一个None值</strong></p>
<p>用例如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    a = torch.tensor(<span class="hljs-number">2.</span>, requires_grad=<span class="hljs-literal">True</span>)<br>    b = torch.tensor(<span class="hljs-number">3.</span>, requires_grad=<span class="hljs-literal">True</span>)<br>    c = a * b<br>    <span class="hljs-built_in">print</span>(c)<br>    c_hook = <span class="hljs-keyword">lambda</span> grad: <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;cc:&quot;</span>, grad)<br>    c.register_hook(c_hook)<br>    d = torch.tensor(<span class="hljs-number">4.</span>, requires_grad=<span class="hljs-literal">True</span>)<br>    d.register_hook(<span class="hljs-keyword">lambda</span> grad: grad * <span class="hljs-number">2</span>)<br>    e = c * d<br>    e.backward()<br>    <span class="hljs-built_in">print</span>(a.grad, b.grad, d.grad)<br><br><span class="hljs-comment"># 输出结果:</span><br>tensor(<span class="hljs-number">6.</span>, grad_fn=&lt;MulBackward0&gt;)<br>cc: tensor(<span class="hljs-number">4.</span>)<br>tensor(<span class="hljs-number">12.</span>) tensor(<span class="hljs-number">8.</span>) tensor(<span class="hljs-number">12.</span>)<br></code></pre></td></tr></table></figure>
<p>上述<code>a,b,d</code>均为计算图中的叶子节点,<code>c,e</code>为中间节点,因此当我们完成反向传递后,计算图会释放中间节点的梯度,我们可以通过上面说的<code>retain_grad()</code>的方式去保留某些<code>tensor</code>的梯度,也可以像上述通过钩子函数,在该张量算完梯度后,把其梯度值钩出来(因为没有retain,反向传播完后还是会释放)</p>
<p>此外,上述也通过lambda表达式对<code>d</code>的梯度值进行了修改,变成了原来的2倍,原本应该是6的,现在<code>d</code>的梯度是12</p>
<h3 id="hook-for-module"><a class="markdownIt-Anchor" href="#hook-for-module"></a> hook for Module</h3>
<p>用于<code>Module</code>的钩子函数在注册的时候返回值也是一个<code>handle</code>,也可以通过<code>handle.remove()</code>移除对应<code>Module</code>的钩子函数,以下对它们各自的钩子函数的声明进行介绍,并且展示一个统一的用例</p>
<p>其中,钩子函数可以用于<code>Module(nn.Module)</code>的子类,如一些基础的算子<code>Conv2d</code>,<code>BatchNorm2d</code>,可以提取它们<code>activation</code>等,也可以将中间层的结果可视化处理</p>
<ul>
<li><code>register_forward_hook(hook)</code>注册的钩子函数是在<code>forward</code>完成后被调用,钩子函数的声明如下:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">hook(module, args, output) -&gt; <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> modified output<br></code></pre></td></tr></table></figure>
<ul>
<li><code>register_forward_pre_hook(hook)</code>注册的钩子函数是在<code>forward</code>执行前被调用,钩子函数的声明如下:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">hook(module, args) -&gt; <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> modified <span class="hljs-built_in">input</span><br></code></pre></td></tr></table></figure>
<ul>
<li><code>register_full_backward_hook(hook)</code>注册的钩子函数在<code>backward</code>完成后被调用,钩子函数的声明如下:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">hook(module, grad_input, grad_output) -&gt; <span class="hljs-built_in">tuple</span>(Tensor) <span class="hljs-keyword">or</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>
<ul>
<li><code>register_full_backward_pre_hook(hook)</code>注册的钩子函数在<code>backward</code>执行前被调用,钩子函数的声明如下:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">hook(module, grad_output) -&gt; <span class="hljs-built_in">tuple</span>[Tensor] <span class="hljs-keyword">or</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>
<p>总用例如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(TestNet, self).__init__()<br>        self.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>)<br>        self.maxpool = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        self._initial_weights()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_initial_weights</span>(<span class="hljs-params">self</span>):</span><br>        self.conv1.weight[<span class="hljs-number">0</span>].data.fill_(<span class="hljs-number">1</span>)<br>        self.conv1.weight[<span class="hljs-number">1</span>].data.fill_(<span class="hljs-number">2</span>)<br>        self.conv1.bias.data.zero_()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># x -&gt; [1,1,4,4]</span><br>        x = self.conv1(x)<br>        <span class="hljs-comment"># x -&gt; [1,2,2,2]</span><br>        x = self.maxpool(x)<br>        <span class="hljs-comment"># x -&gt; [1,2,1,1]</span><br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward_hook</span>(<span class="hljs-params">module_name=<span class="hljs-literal">None</span></span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_forward_hook</span>(<span class="hljs-params">module, <span class="hljs-built_in">input</span>, output</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;---&quot;</span> * <span class="hljs-number">15</span>, <span class="hljs-string">&quot;forward pass&quot;</span>, <span class="hljs-string">&quot;---&quot;</span> * <span class="hljs-number">15</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Module Name: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(module_name))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input Shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">input</span>[<span class="hljs-number">0</span>].shape))  <span class="hljs-comment"># 变成了Tuple,取[0],才是输入的tensor</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">input</span>))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output Shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(output.shape))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(output))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">104</span>)<br>    <span class="hljs-keyword">return</span> _forward_hook<br><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward_hook</span>(<span class="hljs-params">module_name</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_backward_hook</span>(<span class="hljs-params">module, grad_input, grad_output</span>):</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;---&quot;</span> * <span class="hljs-number">15</span>, <span class="hljs-string">&quot;backward pass&quot;</span>, <span class="hljs-string">&quot;---&quot;</span> * <span class="hljs-number">15</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Module Name: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(module_name))<br>        <span class="hljs-keyword">if</span> grad_input[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Gradient Input Shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(grad_input[<span class="hljs-number">0</span>].shape))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Gradient Input: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(grad_input))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Gradient Output Shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(grad_output[<span class="hljs-number">0</span>].shape))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Gradient Output: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(grad_output))<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;-&#x27;</span> * <span class="hljs-number">105</span>)<br>    <span class="hljs-keyword">return</span> _backward_hook<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    model = TestNet()<br>    loss_function = nn.MSELoss()<br>    optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">.0001</span>)<br><br>    forward_hooks_dict = &#123;&#125;<br>    backward_hooks_dict = &#123;&#125;<br><br>    <span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> model.named_modules():<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Module) <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>:  <span class="hljs-comment"># 确保取到的模块可以注册hook</span><br>            <span class="hljs-keyword">continue</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(module._modules) == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 确保取到的模块是子模块,而非中间的模块,如nn.Sequential之类的</span><br>            f_handle = module.register_forward_hook(forward_hook(name))<br>            forward_hooks_dict[name] = f_handle  <span class="hljs-comment"># 存入对应dict,便于之后remove对应name的hook</span><br>            b_handle = module.register_full_backward_hook(backward_hook(name))<br>            backward_hooks_dict[name] = b_handle<br><br>    fake_data = torch.ones([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>])<br>    fake_label = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=torch.<span class="hljs-built_in">float</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;fake label shape: &#123;&#125;\nfake label:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(fake_label.shape, fake_label))<br>    y_logit = model(fake_data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_logit shape:&#123;&#125;\ny_logit&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(y_logit.shape, y_logit))<br>    loss = loss_function(y_logit, fake_label)<br>    optimizer.zero_grad()<br>    loss.backward()<br>    optimizer.step()<br></code></pre></td></tr></table></figure>
<p>运行结果如下图:</p>
<p><img src="https://s2.loli.net/2023/12/10/vFOdzJ5gT4Pt2eY.png" srcset="/img/loading.gif" lazyload alt="forward_backward_hook_example" /></p>
<p>具体使用可以参考:<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook">pytorch-hook使用指南</a></p>
<h2 id="dataloader中的num_worker和torch中的set_num_threads"><a class="markdownIt-Anchor" href="#dataloader中的num_worker和torch中的set_num_threads"></a> DataLoader中的num_worker和torch中的set_num_threads</h2>
<h3 id="先说说一些cpu的概念"><a class="markdownIt-Anchor" href="#先说说一些cpu的概念"></a> 先说说一些CPU的概念</h3>
<p>通过以下指令查看<strong>物理CPU数目</strong>:</p>
<p><code>cat /proc/cpuinfo | grep 'physical id'| sort | uniq | wc -l </code></p>
<p>通过以下指令查看<strong>每个CPU的核心数</strong>:</p>
<p><code>cat /proc/cpuinfo | grep 'core id' | sort | uniq | wc -l</code></p>
<p>以上,我们就可以计算出<strong>CPU的总核心数</strong></p>
<p><code>总核心数=物理CPU数目*每个CPU的核心数</code></p>
<p>而一般来说,一个核心就对应一个物理线程,而有个叫<strong>超线程</strong>的技术,可以把一个核心当作两个线程来用,也就相当于像两个核心一样.而总逻辑CPU数就是在总核心数的基础上乘上超线程的倍数</p>
<p><code>总逻辑CPU数目=物理CPU数目*每个CPU的核心数*超线程系数</code></p>
<p>可以通过以下指令查看<strong>总逻辑CPU数目</strong></p>
<p><code>cat /proc/cpuinfo | grep 'processor' |sort | uniq | wc -l </code></p>
<p>关于逻辑CPU的体现,比如<code>top</code>指令中的<code>%CPU</code>表示的是占用的逻辑CPU数目</p>
<h3 id="关于torchset_num_threads"><a class="markdownIt-Anchor" href="#关于torchset_num_threads"></a> 关于torch.set_num_threads()</h3>
<p>这个<strong>线程数默认是CPU核心总数</strong>,可以通过<code>torch.get_num_threads()</code>获得,且<strong>一般默认的运算效率是最高的</strong></p>
<p>需要设置这个的场景是当<strong>多人共享CPU资源进行模型运算</strong>时用的,<strong>以避免一个进程抢占过多的CPU核心</strong></p>
<p>除了通过<code>torch.set_num_threads()</code>设置,还可以通过环境变量设置,如:<code>MKL_NUM_THREADS和OMP_NUM_THREADS</code>来设置,它们的优先级如下:<code>torch.set_num_threads() &gt; MKL_NUM_THREADS &gt; OMP_NUM_THREADS</code></p>
<p>其中<strong>一般要运用到这个设置是利用CPU进行大量张量操作</strong>,若是大部分的张量操作都是在GPU上,那设置这个也没啥用,且设置的时候由于PyTorch文档没有说哪些运算会从这个设置上受益,因此建议一边看着CPU利用率一边调整线程数,以最大化CPU利用率</p>
<p>这些设置的线程应该是用于<strong>算子内并行</strong>(intra-op parallelism)的</p>
<h3 id="关于dataloader中的参数num_worker"><a class="markdownIt-Anchor" href="#关于dataloader中的参数num_worker"></a> 关于DataLoader中的参数num_worker</h3>
<p>DataLoader中的num_worker是用于指定加载数据和执行变换的并行worker的数目.如果你在加载很大的图片或者有着复杂的变换操作时,即是此时你的GPU处理数据很快,但是你的DataLoader喂数据给GPU很慢而导致不能连续feed GPU,这种情况下就可以设置较多的worker来解决问题</p>
<p>一般对num_worker的设置是直到epoch中的一个step是足够快的(也就是数据可以及时喂给GPU)</p>
<p>注意:num_worker用到的也是计算机的CPU核心数</p>
<p>本部分内容大多学习自:<a target="_blank" rel="noopener" href="https://blog.csdn.net/a_piece_of_ppx/article/details/123714865">ddp中的核心数和线程数</a>、<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/394952301/answer/1225338415">pytorch模型在multiprocessing下前馈速度明显降低的原因是什么？</a></p>
<p><mark>疑惑点:如果我是CPU+GPU计算,那么设置num_worker加速数据读取就好,那如果我是纯CPU计算,我默认我set_num_threads用的是CPU所有核心数,那我num_worker会抢占资源嘛,还是用的是超线程的(如果有)</mark></p>
<h2 id="trainevalno_grad三者区别"><a class="markdownIt-Anchor" href="#trainevalno_grad三者区别"></a> train(),eval(),no_grad()三者区别</h2>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/pytorch/">pytorch</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/03/14/Some-lib-of-Python/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Some lib of Python</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"1db059241d978c80eecd","clientSecret":"fa4f54290b2645c1dd4af84d9241dcb8c3e8e637","repo":"ayyBlog","owner":"ayyha","admin":["ayyha"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: 'c328aedca97cfd6c60b22049636d6e0a'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- KaTeX -->
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css" />
  





  <script  src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>




  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?9c7ed39aa5906acb06d9f9cb7df236ae";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>

  <script type="text/javascript" src="/js/funnyTitle.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300,"hOffset":30,"vOffset":-50,"superSample":2},"mobile":{"show":false},"react":{"opacityDefault":0.7,"opacityOnHover":0.2}});</script></body>
</html>
