

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="pytorch知识学习">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <meta name="description" content="pytorch知识学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Some knowledge of Pytorch">
<meta property="og:url" content="http://example.com/2023/03/14/Some-knowledge-of-Pytorch/index.html">
<meta property="og:site_name" content="ayyHA&#39;s blog">
<meta property="og:description" content="pytorch知识学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn%20arch.jpg">
<meta property="og:image" content="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn-children-arch.jpg">
<meta property="og:image" content="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn-children-arch-0.jpg">
<meta property="og:image" content="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn-modules-arch.png">
<meta property="og:image" content="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/sequential-arch.png">
<meta property="og:image" content="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/sequential-arch-orderedDict.png">
<meta property="og:image" content="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/modulelist-arch.png">
<meta property="article:published_time" content="2023-03-13T16:25:38.000Z">
<meta property="article:modified_time" content="2023-03-28T10:14:36.364Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn%20arch.jpg">
  
  <title>Some knowledge of Pytorch - ayyHA&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.12","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"9c7ed39aa5906acb06d9f9cb7df236ae","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ayyHA</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Some knowledge of Pytorch">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2023-03-14 00:25" pubdate>
        2023年3月14日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      7.2k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      22 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Some knowledge of Pytorch</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：几秒前
                
              </p>
            
            <div class="markdown-body">
              <h2 id="torch中涉及到的一些函数记录"><a class="markdownIt-Anchor" href="#torch中涉及到的一些函数记录"></a> torch中涉及到的一些函数记录</h2>
<h3 id=""><a class="markdownIt-Anchor" href="#"></a> </h3>
<h2 id="torchnnmodule中的modules和children的区别"><a class="markdownIt-Anchor" href="#torchnnmodule中的modules和children的区别"></a> torch.nn.Module中的modules()和children()的区别</h2>
<p>首先构建一个全连接网络,看看它的结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_dim, hidden_dim1, hidden_dim2, hidden_dim3, out_dim</span>):</span><br>        <span class="hljs-built_in">super</span>(TestNet, self).__init__()<br>        self.layer1 = nn.Sequential(<br>            nn.Linear(in_dim, hidden_dim1),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        )<br>        self.layer2 = nn.Sequential(<br>            nn.Linear(hidden_dim1, hidden_dim2),<br>            nn.Sigmoid()<br>        )<br>        self.layer3 = nn.Sequential(<br>            nn.Linear(hidden_dim2, hidden_dim3),<br>            nn.Tanh()<br>        )<br>        self.layer4 = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.layer4(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    net = TestNet(<span class="hljs-number">5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># net arch</span><br>    <span class="hljs-built_in">print</span>(net)<br></code></pre></td></tr></table></figure>
<p>其结构为:</p>
<p><img src="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn%20arch.jpg" srcset="/img/loading.gif" lazyload alt="fcn arch" /></p>
<p>其结构图大致如下(Sequential以layer的标识号区别之,别的亦同)</p>
<pre class="mermaid">graph
TestNet-->Sequential1 & Sequential2 & Sequential3 & ReLU
Sequential1 --> Linear1 & ReLU1
Sequential2 --> Linear2 & Sigmoid
Sequential3 --> Linear3 & Tanh
%% layer4 --> ReLU4</pre>
<p>而当我们利用<code>net.children()</code>打印时,发现其是generator类型,也即是iterator类型,因此可以通过循环将其输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    net = TestNet(<span class="hljs-number">5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># children arch</span><br>    <span class="hljs-keyword">for</span> i, ele <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(net.children()):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i, ele))<br></code></pre></td></tr></table></figure>
<p>其输出结果如下:</p>
<p><img src="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn-children-arch.jpg" srcset="/img/loading.gif" lazyload alt="fcn-children-arch" /></p>
<p>可见<strong>通过children()获得的结构仅包含最外一层</strong>,也即可以通过如下方式获得其最外层:<code>print(list(net.children())[0])</code>,即可以获得第0个Sequential:</p>
<p><img src="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn-children-arch-0.jpg" srcset="/img/loading.gif" lazyload alt="fcn-children-arch-0" /></p>
<p>而通过modules()获得的也是generator类型,因此也用循环将其输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    net = TestNet(<span class="hljs-number">5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># modules arch</span><br>    <span class="hljs-keyword">for</span> i,ele <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(net.modules()):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;:&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i,ele))<br></code></pre></td></tr></table></figure>
<p>其输出结果为:</p>
<p><img src="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/fcn-modules-arch.png" srcset="/img/loading.gif" lazyload alt="fcn-modules-arch" /></p>
<p>可见,其结果类似于深搜,直接把整个结构DFS了一遍</p>
<p>因此可得<code>children()</code>和<code>modules()</code>的区别如下:</p>
<ul>
<li>通过<code>children()</code>获取网络层级结构,只会取最外层,即根节点下的一层</li>
<li>通过<code>modules()</code>获取网络层级结构,则类似对网络结构进行DFS,依次输出</li>
</ul>
<h2 id="pytorch中的模型容器"><a class="markdownIt-Anchor" href="#pytorch中的模型容器"></a> pytorch中的模型容器</h2>
<p>利用模型容器,可以<strong>自动的将module注册到网络上</strong>,以及<strong>将module的parameters添加到网络上</strong></p>
<h3 id="nnsequential"><a class="markdownIt-Anchor" href="#nnsequential"></a> nn.Sequential</h3>
<p>将如<code>Conv2d</code>,<code>BatchNorm2d</code>,<code>ReLU</code>等的module放入<code>nn.Sequential</code>容器中,将会<strong>按照放置的顺序执行</strong>,e.g.:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br>layer1 = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>),<br>    nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>    nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>),<br>    nn.BatchNorm2d(<span class="hljs-number">512</span>),<br>    nn.ReLU()<br>)<br><span class="hljs-built_in">print</span>(layer1)<br></code></pre></td></tr></table></figure>
<p>其网络层级结构如下所示</p>
<p><img src="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/sequential-arch.png" srcset="/img/loading.gif" lazyload alt="sequential-arch" /></p>
<p>其还可以用<code>OrderedDict</code>去为每个module命名,e.g.:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> collections<br><br>layer1 = nn.Sequential(collections.OrderedDict([<br>    (<span class="hljs-string">&quot;conv1&quot;</span>, nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)),<br>    (<span class="hljs-string">&quot;bn1&quot;</span>, nn.BatchNorm2d(<span class="hljs-number">256</span>)),<br>    (<span class="hljs-string">&quot;relu1&quot;</span>, nn.ReLU()),<br>    (<span class="hljs-string">&quot;conv2&quot;</span>, nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)),<br>    (<span class="hljs-string">&quot;bn2&quot;</span>, nn.BatchNorm2d(<span class="hljs-number">512</span>)),<br>    (<span class="hljs-string">&quot;relu2&quot;</span>, nn.ReLU())<br>]))<br><span class="hljs-built_in">print</span>(layer1)<br></code></pre></td></tr></table></figure>
<p>网络层级结构如下示</p>
<p><img src="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/sequential-arch-orderedDict.png" srcset="/img/loading.gif" lazyload alt="sequential-arch-orderedDict" /></p>
<h3 id="nnmodulelist"><a class="markdownIt-Anchor" href="#nnmodulelist"></a> nn.ModuleList</h3>
<p>该模型容器类似于list,较之<code>nn.Sequential</code>可以更灵活的使用,充当了存放module的容器,执行的顺序在<code>forward</code>中自行定义,较之原生的list,则是可以注册module于网络以及对其参数添加进<code>nn.Parameters()</code>中.e.g.:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TestNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-built_in">super</span>(TestNet, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        self.bn1 = nn.BatchNorm2d(<span class="hljs-number">256</span>)<br>        self.relu1 = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.layer1 = [nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">100</span>), nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-number">10</span>)]<br>        self.layer2_ = [nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">200</span>), nn.Linear(<span class="hljs-number">200</span>, <span class="hljs-number">10</span>)]<br>        self.layer2 = nn.ModuleList(self.layer2_)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        x = self.conv1(x)<br>        x = self.bn1(x)<br>        x = self.relu1(x)<br>        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> self.layer1:<br>            x = l(x)<br>        <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> self.layer2:<br>            x = l(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<p>其执行结果表示,用list充当容器,并不能将所需的module注册,而ModuleList则可以,结果如下图示</p>
<p><img src="https://cdn.staticaly.com/gh/ayyHA/imageBed@main/img/modulelist-arch.png" srcset="/img/loading.gif" lazyload alt="modulelist-arch" /></p>
<h2 id="datasetsamplerdataloader三者关系"><a class="markdownIt-Anchor" href="#datasetsamplerdataloader三者关系"></a> Dataset,Sampler,DataLoader三者关系</h2>
<p>当我们需要对数据文件进行解析获得数据或者自己创建的数据集<code>ImageFolder</code>不能够满足我们的要求,我们则需要自定义一个<code>Dataset</code>类</p>
<p>而定义好一个<code>Dataset</code>类后,我们可以通过循环或索引得到对应的一条数据,形如<code>data,target</code>,而<code>Sampler</code>则是对这些一条条的数据进行采样的工具,Pytorch提供的主要有<code>SequentialSampler</code>和<code>RandomSampler</code>,这些采样器采样得到的都是这些数据的索引</p>
<p>显然我们一条条读取数据并不能满足我们的需求,我们更期望的是以batch为单位的读取数据,因而有<code>BatchSampler</code>这么一个批采样器,它会对我们Sampler,如<code>SequentialSampler</code>采样得到的一个个索引整理成一个<code>batch_size</code>大小的索引序列</p>
<p>在DataLoader里面,我们便是对<code>BatchSampler</code>这个批采样器采样得到的索引序列进行处理,通过传入的<code>dataset</code>参数读取一条条数据,整理成一个<code>List[Tuple[Tensor,Tensor]]</code>这么一个<code>batch_list</code>的形式,交由<code>collate_fn</code>对这一个<code>batch_size</code>大小的数据进行整理,而后得到我们在循环里对<code>DataLoader</code>遍历的数据</p>
<p>以下展现的是工作进程(num_worker)为0的<code>DataLoader</code>处理方式(跟我上面说的流程一样):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># dataloader.py:</span><br> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DataLoader</span>(<span class="hljs-params"><span class="hljs-type">Generic</span>[T_co]</span>):</span><br> <span class="hljs-comment"># ...</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_iterator</span>(<span class="hljs-params">self</span>) -&gt; &#x27;_BaseDataLoaderIter&#x27;:</span><br>        <span class="hljs-keyword">if</span> self.num_workers == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> _SingleProcessDataLoaderIter(self)<br> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_SingleProcessDataLoaderIter</span>(<span class="hljs-params">_BaseDataLoaderIter</span>):</span><br> <span class="hljs-comment"># ...</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_next_data</span>(<span class="hljs-params">self</span>):</span><br>        index = self._next_index()  <span class="hljs-comment"># may raise StopIteration</span><br>        data = self._dataset_fetcher.fetch(index)  <span class="hljs-comment"># may raise StopIteration</span><br>        <span class="hljs-keyword">if</span> self._pin_memory:<br>            data = _utils.pin_memory.pin_memory(data)<br>        <span class="hljs-keyword">return</span> data<br><br><span class="hljs-comment"># fetch.py</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">_MapDatasetFetcher</span>(<span class="hljs-params">_BaseDatasetFetcher</span>):</span><br> <span class="hljs-comment"># ...</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch</span>(<span class="hljs-params">self, possibly_batched_index</span>):</span><br>        <span class="hljs-keyword">if</span> self.auto_collation:<br>            data = [self.dataset[idx] <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> possibly_batched_index]<br>        <span class="hljs-keyword">else</span>:<br>            data = self.dataset[possibly_batched_index]<br>        <span class="hljs-keyword">return</span> self.collate_fn(data)<br></code></pre></td></tr></table></figure>
<p>自定义一个<code>Dataset</code>类需要的工作:</p>
<ul>
<li>继承<code>torch.utils.data.Dataset</code></li>
<li>在<code>__init__</code>中传入需要处理的数据(可能是数据的目录啥的),对数据的预处理方法</li>
<li>在**<code>__getitem__</code>**中完成对数据的解析,预处理,然后返回对应的数据,如<code>return data,target</code></li>
<li>在<code>__len__</code>中反应需要处理的数据集的大小</li>
</ul>
<p>自定义一个Sampler类需要的工作:</p>
<ul>
<li>继承<code>torch.utils.data.Sampler</code></li>
<li>在<code>__iter__</code>方法中返回一个iterator</li>
</ul>
<h2 id="torchutilsdatadataloader中的collate_fn"><a class="markdownIt-Anchor" href="#torchutilsdatadataloader中的collate_fn"></a> torch.utils.data.DataLoader中的collate_fn</h2>
<p>在涉及到数据集处理的时候,我们需要考虑到<code>torch.utils.data.Dataset</code>类,以及<code>torch.utils.data.DataLoader</code>类</p>
<p>其中前者是我们对数据集的处理,比如解析数据集数据,然后<strong>重写<code>__len__()</code>以及<code>__getitem__()</code>方法</strong>,而后在其中对数据进行<strong>预处理</strong>操作,然后才会得到**<code>data,target</code><strong>这样的数据,其中<code>data</code>可以是图像之类的,而<code>target</code>则可能是标签,GTbox的位置等信息,这时我们通过<code>__getitem__</code>方法</strong>只是获取到一条<code>data,target</code>数据**,处理批量数据的任务则由<code>DataLoader</code>承担</p>
<p>我们在不考虑这一部分将要提及的<code>collate_fn</code>方法前,看一看<code>DataLoader</code>是怎么样将一条数据变成一个<code>batch_size</code>的数据的</p>
<p>以下是代码部分:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data<br>data = torch.tensor([<br>    [<span class="hljs-number">0.4698</span>, <span class="hljs-number">0.6971</span>, <span class="hljs-number">0.9499</span>, <span class="hljs-number">0.3641</span>],<br>    [<span class="hljs-number">0.0896</span>, <span class="hljs-number">0.5345</span>, <span class="hljs-number">0.5603</span>, <span class="hljs-number">0.5409</span>],<br>    [<span class="hljs-number">0.4988</span>, <span class="hljs-number">0.2155</span>, <span class="hljs-number">0.1244</span>, <span class="hljs-number">0.3456</span>],<br>    [<span class="hljs-number">0.4812</span>, <span class="hljs-number">0.0108</span>, <span class="hljs-number">0.1885</span>, <span class="hljs-number">0.8593</span>],<br>    [<span class="hljs-number">0.6564</span>, <span class="hljs-number">0.3428</span>, <span class="hljs-number">0.8815</span>, <span class="hljs-number">0.3558</span>]])<br>target = torch.tensor([<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>])<br>dataset = Data.TensorDataset(data, target)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> dataset:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i&#125;</span>&quot;</span>)<br><br>batch_size = <span class="hljs-number">2</span><br>dataloader = Data.DataLoader(batch_size=batch_size, dataset=dataset)<br><br><span class="hljs-keyword">for</span> d, t <span class="hljs-keyword">in</span> dataloader:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;data:<span class="hljs-subst">&#123;d&#125;</span>\ntarget:<span class="hljs-subst">&#123;t&#125;</span>\n&quot;</span>)<br><span class="hljs-comment"># 输出结果:</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([0.4698, 0.6971, 0.9499, 0.3641]), tensor(4))</span><br><span class="hljs-string">(tensor([0.0896, 0.5345, 0.5603, 0.5409]), tensor(4))</span><br><span class="hljs-string">(tensor([0.4988, 0.2155, 0.1244, 0.3456]), tensor(1))</span><br><span class="hljs-string">(tensor([0.4812, 0.0108, 0.1885, 0.8593]), tensor(3))</span><br><span class="hljs-string">(tensor([0.6564, 0.3428, 0.8815, 0.3558]), tensor(1))</span><br><span class="hljs-string">data:tensor([[0.4698, 0.6971, 0.9499, 0.3641],</span><br><span class="hljs-string">        [0.0896, 0.5345, 0.5603, 0.5409]])</span><br><span class="hljs-string">target:tensor([4, 4])</span><br><span class="hljs-string"></span><br><span class="hljs-string">data:tensor([[0.4988, 0.2155, 0.1244, 0.3456],</span><br><span class="hljs-string">        [0.4812, 0.0108, 0.1885, 0.8593]])</span><br><span class="hljs-string">target:tensor([1, 3])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>
<p>可见,<code>dataset</code>的结构应是一个<code>Sequential[Tuple[Tensor,Tensor]]</code>,而其中一条数据中的tuple里面则包含了<code>data</code>以及<code>target</code>,也即是说通过<code>Dataset</code>我们逐条获取数据得到的是形如<code>(data,target)</code>这样的数据</p>
<p>而显然我们更加希望<code>data</code>归为<code>data</code>,而<code>target</code>归为<code>target</code>,就如用普通的<code>DataLoader</code>得到的结果一样,输出的结果是一个batch的data和一个batch的target</p>
<p>显然<code>DataLoader</code>在其内部即帮我们完成了①按batch_size划分数据;②<code>data</code>为<code>data</code>,<code>target</code>为<code>target</code></p>
<p>以下,我们采用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y=x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>的形式于<code>collate_fn</code>看下输出的结果是什么</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">loader = Data.DataLoader(batch_size=batch_size, dataset=dataset, collate_fn=<span class="hljs-keyword">lambda</span> x: x)<br>it = <span class="hljs-built_in">iter</span>(loader)<br>batch_data = <span class="hljs-built_in">next</span>(it)<br><span class="hljs-built_in">print</span>(batch_data)<br><span class="hljs-comment"># 输出结果</span><br><span class="hljs-comment"># [(tensor([0.4698, 0.6971, 0.9499, 0.3641]), tensor(4)), (tensor([0.0896, 0.5345, 0.5603, 0.5409]), tensor(4))]</span><br></code></pre></td></tr></table></figure>
<p>根据输出结果不难看出: 在进入<code>collate_fn</code>之前,数据已经按<code>batch_size</code>划分好了,其结构为:<code>List[Tuple(Tensor,Tensor)]</code>,其中List的大小是batch_size的大小,但是数据格式依旧是<code>dataset</code>的结构,因而<code>collate_fn</code>这个方法是用来<strong>调整数据格式</strong>的,在我们不调用自定义的<code>collate_fn</code>时,会用系统默认的函数,将输出调整为batch_size大小的data和target这两个部分</p>
<p>以下是等价于系统默认的<code>collate_fn</code>(能将输出划分为data和target两部分):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># input x: List[Tuple[Tensor,Tensor],...]</span><br>collate_func = <span class="hljs-keyword">lambda</span> x:(<br>	torch.cat(<br>        <span class="hljs-comment"># data: [4] -&gt; [1,4] -&gt; [N,4]</span><br>        <span class="hljs-comment"># target: [1] -&gt; [1,1] -&gt; [N,1]</span><br>        [x[i][j].unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x))],<span class="hljs-number">0</span><br>    ) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x[<span class="hljs-number">0</span>]))<br>)<br></code></pre></td></tr></table></figure>
<p>以上是分别对data,target数据进行取出,然后扩充维度,而后对扩充的维度进行拼接,便得到了期望的结果</p>
<p>一般来说,不会用到它,但<mark>我在网上以及个人思考后</mark>(pytorch这方面源码没看懂😂),应该是通过<code>torch.stack()</code>进行的维度堆叠,因而如果图片的尺寸或者target中如标签的数目不等,则需要自定义</p>
<p>比如说一个batch_size是2,那两张图片分别是有两个和三个目标,即对应的target为<code>[2,5]/[3,5]</code>无法用<code>stack()</code>对它们简单的拼接,因而需要自己定义一个<code>collate_fn</code>去处理这个问题,自己去定义一个batch出来的数据的格式</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/pytorch/">pytorch</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/03/14/Some-lib-of-Python/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Some lib of Python</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="gitalk-container"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#gitalk-container', function() {
      Fluid.utils.createCssLink('/css/gitalk.css')
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', function() {
        var options = Object.assign(
          {"clientID":"1db059241d978c80eecd","clientSecret":"fa4f54290b2645c1dd4af84d9241dcb8c3e8e637","repo":"ayyBlog","owner":"ayyha","admin":["ayyha"],"language":"zh-CN","labels":["Gitalk"],"perPage":10,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token"},
          {
            id: 'c328aedca97cfd6c60b22049636d6e0a'
          }
        )
        var gitalk = new Gitalk(options);
        gitalk.render('gitalk-container');
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- KaTeX -->
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css" />
  





  <script  src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>




  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?9c7ed39aa5906acb06d9f9cb7df236ae";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300,"hOffset":30,"vOffset":-50,"superSample":2},"mobile":{"show":false},"react":{"opacityDefault":0.7,"opacityOnHover":0.2}});</script></body>
</html>
